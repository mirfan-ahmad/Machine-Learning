{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c64677ff",
   "metadata": {},
   "source": [
    "## Goals\n",
    "In this lab, you will:\n",
    "- automate the process of optimizing $w$ and $b$ using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b934509",
   "metadata": {},
   "source": [
    "## Tools\n",
    "In this lab, we will make use of: \n",
    "- NumPy, a popular library for scientific computing\n",
    "- Matplotlib, a popular library for plotting data\n",
    "- plotting routines in the lab_utils.py file in the local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3b9a0ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lab_utils_uni'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlab_utils_uni\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lab_utils_uni'"
     ]
    }
   ],
   "source": [
    "import math, copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lab_utils_uni import plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad849d1",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2\"></a>\n",
    "# Problem Statement\n",
    "\n",
    "Let's use the same two data points as before - a house with 1000 square feet sold for \\\\$300,000 and a house with 2000 square feet sold for \\\\$500,000.\n",
    "\n",
    "| Size (1000 sqft)     | Price (1000s of dollars) |\n",
    "| ----------------| ------------------------ |\n",
    "| 1               | 300                      |\n",
    "| 2               | 500                      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84669c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our data set\n",
    "x_train = np.array([1.0, 2.0])   #features\n",
    "y_train = np.array([300.0, 500.0])   #target value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14625ea",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.0.1\"></a>\n",
    "### Compute_Cost\n",
    "This was developed in the last lab. We'll need it again here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40d37fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate the cost\n",
    "def compute_cost(x, y, w, b):\n",
    "   \n",
    "    m = x.shape[0] \n",
    "    cost = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = cost + (f_wb - y[i])**2\n",
    "    total_cost = 1 / (2 * m) * cost\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbc45a6",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.1\"></a>\n",
    "## Gradient descent summary\n",
    "So far in this course, you have developed a linear model that predicts $f_{w,b}(x^{(i)})$:\n",
    "$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "In linear regression, you utilize input training data to fit the parameters $w$,$b$ by minimizing a measure of the error between our predictions $f_{w,b}(x^{(i)})$ and the actual data $y^{(i)}$. The measure is called the $cost$, $J(w,b)$. In training you measure the cost over all of our training samples $x^{(i)},y^{(i)}$\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee618a5c",
   "metadata": {},
   "source": [
    "\n",
    "In lecture, *gradient descent* was described as:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "where, parameters $w$, $b$ are updated simultaneously.  \n",
    "The gradient is defined as:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here *simultaniously* means that you calculate the partial derivatives for all the parameters before updating any of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d58ac0",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.2\"></a>\n",
    "## Implement Gradient Descent\n",
    "You will implement gradient descent algorithm for one feature. You will need three functions. \n",
    "- `compute_gradient` implementing equation (4) and (5) above\n",
    "- `compute_cost` implementing equation (2) above (code from previous lab)\n",
    "- `gradient_descent`, utilizing compute_gradient and compute_cost\n",
    "\n",
    "Conventions:\n",
    "- The naming of python variables containing partial derivatives follows this pattern,$\\frac{\\partial J(w,b)}{\\partial b}$  will be `dj_db`.\n",
    "- w.r.t is With Respect To, as in partial derivative of $J(wb)$ With Respect To $b$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814d8e91",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.3\"></a>\n",
    "### compute_gradient\n",
    "<a name='ex-01'></a>\n",
    "`compute_gradient`  implements (4) and (5) above and returns $\\frac{\\partial J(w,b)}{\\partial w}$,$\\frac{\\partial J(w,b)}{\\partial b}$. The embedded comments describe the operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f972c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      y (ndarray (m,)): target values\n",
    "      w,b (scalar)    : model parameters  \n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
    "     \"\"\"\n",
    "    \n",
    "    # Number of training examples\n",
    "    m = x.shape[0]    \n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):  \n",
    "        f_wb = w * x[i] + b \n",
    "        dj_dw_i = (f_wb - y[i]) * x[i] \n",
    "        dj_db_i = f_wb - y[i] \n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i \n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb00a39",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd5dc922",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt_gradients' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt_gradients(x_train,y_train, compute_cost, compute_gradient)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt_gradients' is not defined"
     ]
    }
   ],
   "source": [
    "plt_gradients(x_train,y_train, compute_cost, compute_gradient)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f48de",
   "metadata": {},
   "source": [
    "Above, the left plot shows $\\frac{\\partial J(w,b)}{\\partial w}$ or the slope of the cost curve relative to $w$ at three points. On the right side of the plot, the derivative is positive, while on the left it is negative. Due to the 'bowl shape', the derivatives will always lead gradient descent toward the bottom where the gradient is zero.\n",
    " \n",
    "The left plot has fixed $b=100$. Gradient descent will utilize both $\\frac{\\partial J(w,b)}{\\partial w}$ and $\\frac{\\partial J(w,b)}{\\partial b}$ to update parameters. The 'quiver plot' on the right provides a means of viewing the gradient of both parameters. The arrow sizes reflect the magnitude of the gradient at that point. The direction and slope of the arrow reflects the ratio of $\\frac{\\partial J(w,b)}{\\partial w}$ and $\\frac{\\partial J(w,b)}{\\partial b}$ at that point.\n",
    "Note that the gradient points *away* from the minimum. Review equation (3) above. The scaled gradient is *subtracted* from the current value of $w$ or $b$. This moves the parameter in a direction that will reduce cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e445203",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.5\"></a>\n",
    "###  Gradient Descent\n",
    "Now that gradients can be computed,  gradient descent, described in equation (3) above can be implemented below in `gradient_descent`. The details of the implementation are described in the comments. Below, you will utilize this function to find optimal values of $w$ and $b$ on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7303aeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): \n",
    "    \"\"\"\n",
    "    Performs gradient descent to fit w,b. Updates w,b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,))  : Data, m examples \n",
    "      y (ndarray (m,))  : target values\n",
    "      w_in,b_in (scalar): initial values of model parameters  \n",
    "      alpha (float):     Learning rate\n",
    "      num_iters (int):   number of iterations to run gradient descent\n",
    "      cost_function:     function to call to produce cost\n",
    "      gradient_function: function to call to produce gradient\n",
    "      \n",
    "    Returns:\n",
    "      w (scalar): Updated value of parameter after running gradient descent\n",
    "      b (scalar): Updated value of parameter after running gradient descent\n",
    "      J_history (List): History of cost values\n",
    "      p_history (list): History of parameters [w,b] \n",
    "      \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w , b)     \n",
    "\n",
    "        # Update Parameters using equation (3) above\n",
    "        b = b - alpha * dj_db                            \n",
    "        w = w - alpha * dj_dw                            \n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(x, y, w , b))\n",
    "            p_history.append([w,b])\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    " \n",
    "    return w, b, J_history, p_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c34c59a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m tmp_alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0e-2\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# run gradient descent\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m w_final, b_final, J_hist, p_hist \u001b[38;5;241m=\u001b[39m gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n\u001b[1;32m      9\u001b[0m                                                     iterations, compute_cost, compute_gradient)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(w,b) found by gradient descent: (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw_final\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m8.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb_final\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m8.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "tmp_alpha = 1.0e-2\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n",
    "                                                    iterations, compute_cost, compute_gradient)\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93d8289",
   "metadata": {},
   "source": [
    "### Cost versus iterations of gradient descent \n",
    "A plot of cost versus iterations is a useful measure of progress in gradient descent. Cost should always decrease in successful runs. The change in cost is so rapid initially, it is useful to plot the initial decent on a different scale than the final descent. In the plots below, note the scale of cost on the axes and the iteration step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78f47d57",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'J_hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# plot cost versus iteration  \u001b[39;00m\n\u001b[1;32m      2\u001b[0m fig, (ax1, ax2) \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, constrained_layout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m ax1\u001b[38;5;241m.\u001b[39mplot(J_hist[:\u001b[38;5;241m100\u001b[39m])\n\u001b[1;32m      4\u001b[0m ax2\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(J_hist[\u001b[38;5;241m1000\u001b[39m:])), J_hist[\u001b[38;5;241m1000\u001b[39m:])\n\u001b[1;32m      5\u001b[0m ax1\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCost vs. iteration(start)\u001b[39m\u001b[38;5;124m\"\u001b[39m);  ax2\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCost vs. iteration (end)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'J_hist' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot cost versus iteration  \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))\n",
    "ax1.plot(J_hist[:100])\n",
    "ax2.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:])\n",
    "ax1.set_title(\"Cost vs. iteration(start)\");  ax2.set_title(\"Cost vs. iteration (end)\")\n",
    "ax1.set_ylabel('Cost')            ;  ax2.set_ylabel('Cost') \n",
    "ax1.set_xlabel('iteration step')  ;  ax2.set_xlabel('iteration step') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abdd296",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "Now that you have discovered the optimal values for the parameters $w$ and $b$, you can now use the model to predict housing values based on our learned parameters. As expected, the predicted values are nearly the same as the training values for the same housing. Further, the value not in the prediction is in line with the expected value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f342a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"1000 sqft house prediction {w_final*1.0 + b_final:0.1f} Thousand dollars\")\n",
    "print(f\"1200 sqft house prediction {w_final*1.2 + b_final:0.1f} Thousand dollars\")\n",
    "print(f\"2000 sqft house prediction {w_final*2.0 + b_final:0.1f} Thousand dollars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827923fd",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.6\"></a>\n",
    "## Plotting\n",
    "You can show the progress of gradient descent during its execution by plotting the cost over iterations on a contour plot of the cost(w,b). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4397d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12, 6))\n",
    "plt_contour_wgrad(x_train, y_train, p_hist, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8b0ac4",
   "metadata": {},
   "source": [
    "Above, the contour plot shows the $cost(w,b)$ over a range of $w$ and $b$. Cost levels are represented by the rings. Overlayed, using red arrows, is the path of gradient descent. Here are some things to note:\n",
    "- The path makes steady (monotonic) progress toward its goal.\n",
    "- initial steps are much larger than the steps near the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871b8ba4",
   "metadata": {},
   "source": [
    "**Zooming in**, we can see that final steps of gradient descent. Note the distance between steps shrinks as the gradient approaches zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4153bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12, 4))\n",
    "plt_contour_wgrad(x_train, y_train, p_hist, ax, w_range=[180, 220, 0.5], b_range=[80, 120, 0.5],\n",
    "            contours=[1,5,10,20],resolution=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bea6608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# set alpha to a large value\n",
    "iterations = 10\n",
    "tmp_alpha = 8.0e-1\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n",
    "                                                    iterations, compute_cost, compute_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c011d1",
   "metadata": {},
   "source": [
    "Above, $w$ and $b$ are bouncing back and forth between positive and negative with the absolute value increasing with each iteration. Further, each iteration $\\frac{\\partial J(w,b)}{\\partial w}$ changes sign and cost is increasing rather than decreasing. This is a clear sign that the *learning rate is too large* and the solution is diverging. \n",
    "Let's visualize this with a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f8cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_divergence(p_hist, J_hist,x_train, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b03b2",
   "metadata": {},
   "source": [
    "Above, the left graph shows $w$'s progression over the first few steps of gradient descent. $w$ oscillates from positive to negative and cost grows rapidly. Gradient Descent is operating on both $w$ and $b$ simultaneously, so one needs the 3-D plot on the right for the complete picture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0458f87",
   "metadata": {},
   "source": [
    "\n",
    "## Congratulations!\n",
    "In this lab you:\n",
    "- delved into the details of gradient descent for a single variable.\n",
    "- developed a routine to compute the gradient\n",
    "- visualized what the gradient is\n",
    "- completed a gradient descent routine\n",
    "- utilized gradient descent to find parameters\n",
    "- examined the impact of sizing the learning rate"
   ]
  }
 ],
 "metadata": {
  "dl_toc_settings": {
   "rndtag": "40291"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
